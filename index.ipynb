{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-biasing Transcribed Text from Automatic Speech Recognition Systems\n",
    "# Copyright (C) 2021  Rigved Rakshit\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as published\n",
    "# by the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "# This code has been modified from the following Kaggle Jupyter notebook:\n",
    "# https://www.kaggle.com/huseinzol05/sound-augmentation-librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import wave\n",
    "import random\n",
    "import librosa.display\n",
    "import librosa\n",
    "import tempfile\n",
    "import soundfile\n",
    "import deepspeech\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly select a voice recording from the 100 available instances\n",
    "# These 100 voice recording have been selected from the Mozilla CommonVoice v6.1 English dataset\n",
    "\n",
    "validated_csv_filename = os.path.join('./', 'validated.csv')\n",
    "\n",
    "random.seed()\n",
    "\n",
    "with open(validated_csv_filename, 'r') as validated_csv_file:\n",
    "    reader = csv.DictReader(validated_csv_file)\n",
    "    selected_filename = np.random.choice(os.listdir('./data/'))\n",
    "\n",
    "    for row in reader:\n",
    "        if row['path'] == selected_filename:\n",
    "            selected_filename = os.path.join('./data', row['path'])\n",
    "\n",
    "            print('Selected file has following attributes:\\n')\n",
    "            print('client_id: ' + row['client_id'])\n",
    "            print('path:' + row['path'])\n",
    "            print('sentence: ' + row['sentence'])\n",
    "            ground_truth_sentence = row['sentence'].strip().lower()\n",
    "            print('up_votes:' + row['up_votes'])\n",
    "            print('down_votes: ' + row['down_votes'])\n",
    "            print('age: ' + row['age'])\n",
    "            print('gender: ' + row['gender'])\n",
    "            print('accent: ' + row['accent'])\n",
    "            print('locale: ' + row['locale'])\n",
    "            print('segment: ' + row['segment'])\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the original audio\n",
    "\n",
    "y, sr = librosa.load(selected_filename, sr=16000)\n",
    "\n",
    "# Perform time-stretching on the original audio\n",
    "\n",
    "y_time_stretched = y.copy()\n",
    "tmp_time_stretched = librosa.effects.time_stretch(y, np.random.uniform(0.9, 1.1))\n",
    "minlen = min(y.shape[0], tmp_time_stretched.shape[0])\n",
    "y_time_stretched *= 0\n",
    "y_time_stretched[0:minlen] = tmp_time_stretched[0:minlen]\n",
    "\n",
    "# Perform pitch-shifting on the original audio\n",
    "\n",
    "y_pitch_shifted = librosa.effects.pitch_shift(y, sr, n_steps=(4 * np.random.uniform()))\n",
    "\n",
    "y_louder = y * np.random.uniform(low=1.5, high=3.0)\n",
    "\n",
    "# Save these pre-processed files\n",
    "\n",
    "audio_time_stretched_filename = tempfile.NamedTemporaryFile(suffix='.wav').name\n",
    "audio_pitch_shifted_filename = tempfile.NamedTemporaryFile(suffix='.wav').name\n",
    "audio_louder_filename = tempfile.NamedTemporaryFile(suffix='.wav').name\n",
    "\n",
    "soundfile.write(audio_time_stretched_filename, y_time_stretched, sr, subtype='PCM_16')\n",
    "soundfile.write(audio_pitch_shifted_filename, y_pitch_shifted, sr, subtype='PCM_16')\n",
    "soundfile.write(audio_louder_filename, y_louder, sr, subtype='PCM_16')\n",
    "\n",
    "# Reload these pre-processed files in the format expected by the pre-trained Mozilla DeepSpeech binary\n",
    "\n",
    "with wave.open(selected_filename, 'rb') as audio_input_file:\n",
    "    audio_input = np.frombuffer(\n",
    "        audio_input_file.readframes(\n",
    "            audio_input_file.getnframes()\n",
    "        ), np.int16\n",
    "    )\n",
    "\n",
    "with wave.open(audio_time_stretched_filename, 'rb') as audio_time_stretched_file:\n",
    "    audio_time_stretched = np.frombuffer(\n",
    "        audio_time_stretched_file.readframes(\n",
    "            audio_time_stretched_file.getnframes()\n",
    "        ), np.int16\n",
    "    )\n",
    "\n",
    "with wave.open(audio_pitch_shifted_filename, 'rb') as audio_pitch_shifted_file:\n",
    "    audio_pitch_shifted = np.frombuffer(\n",
    "        audio_pitch_shifted_file.readframes(\n",
    "            audio_pitch_shifted_file.getnframes()\n",
    "        ), np.int16\n",
    "    )\n",
    "\n",
    "with wave.open(audio_louder_filename, 'rb') as audio_louder_file:\n",
    "    audio_louder = np.frombuffer(\n",
    "        audio_louder_file.readframes(\n",
    "            audio_louder_file.getnframes()\n",
    "        ), np.int16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pre-built DeepSpeech model and scorer memory-mapped files\n",
    "\n",
    "ds = deepspeech.Model(os.path.join('./', 'deepspeech-0.9.3-models.pbmm'))\n",
    "ds.enableExternalScorer(os.path.join('./', 'deepspeech-0.9.3-models.scorer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Perform speech-to-text on the 4 audio clips\n",
    "\n",
    "audio_transcription_hypothesis = ds.stt(audio_input)\n",
    "audio_time_stretched_transcription_hypothesis = ds.stt(audio_time_stretched)\n",
    "audio_pitch_shifted_transcription_hypothesis = ds.stt(audio_pitch_shifted)\n",
    "audio_loudness_level_increased_transcription_hypothesis = ds.stt(audio_louder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output on the original audio\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of original audio: ' + audio_transcription_hypothesis)\n",
    "\n",
    "print('Original audio wave-plot:')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio_input.astype('float'), sr=sr)\n",
    "plt.show()\n",
    "\n",
    "Audio(audio_input, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output on time-stretched audio\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of time-stretched audio: ' + audio_time_stretched_transcription_hypothesis)\n",
    "\n",
    "print('Time-stretched audio wave-plot:')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio_time_stretched.astype('float'), sr=sr)\n",
    "plt.show()\n",
    "\n",
    "Audio(audio_time_stretched, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output on the pitch-shifted audio\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of pitch-shifted audio: ' + audio_pitch_shifted_transcription_hypothesis)\n",
    "\n",
    "print('Pitch-shifted audio wave-plot:')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio_pitch_shifted.astype('float'), sr=sr)\n",
    "plt.show()\n",
    "\n",
    "Audio(audio_pitch_shifted, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output on the audio with volume increased\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of louder audio: ' + audio_loudness_level_increased_transcription_hypothesis)\n",
    "\n",
    "print('Louder audio wave-plot:')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio_louder.astype('float'), sr=sr)\n",
    "plt.show()\n",
    "\n",
    "Audio(audio_louder, rate=sr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}