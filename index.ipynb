{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-biasing Transcribed Text from Automatic Speech Recognition Systems\n",
    "# Copyright (C) 2021  Rigved Rakshit\n",
    "#\n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as published\n",
    "# by the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "\n",
    "# This code has been modified from the following Kaggle Jupyter notebook:\n",
    "# https://www.kaggle.com/huseinzol05/sound-augmentation-librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import wave\n",
    "import random\n",
    "import librosa.display\n",
    "import librosa\n",
    "import tempfile\n",
    "import soundfile\n",
    "import deepspeech\n",
    "import numpy as np\n",
    "import language_tool_python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly select a voice recording from the 100 available instances\n",
    "# These 100 voice recording have been selected\n",
    "# from the Mozilla CommonVoice v6.1 English dataset\n",
    "\n",
    "validated_csv_filename = os.path.join('./', 'validated.csv')\n",
    "\n",
    "random.seed()\n",
    "\n",
    "with open(validated_csv_filename, 'r') as validated_csv_file:\n",
    "    reader = csv.DictReader(validated_csv_file)\n",
    "    selected_filename = np.random.choice(os.listdir('./data/'))\n",
    "\n",
    "    for row in reader:\n",
    "        if row['path'] == selected_filename:\n",
    "            selected_filename = os.path.join('./data', row['path'])\n",
    "\n",
    "            print('Selected file has following attributes:\\n')\n",
    "            print('client_id: ' + row['client_id'])\n",
    "            print('path:' + row['path'])\n",
    "            print('sentence: ' + row['sentence'])\n",
    "            ground_truth_sentence = row['sentence'].strip().lower()\n",
    "            print('up_votes:' + row['up_votes'])\n",
    "            print('down_votes: ' + row['down_votes'])\n",
    "            print('age: ' + row['age'])\n",
    "            print('gender: ' + row['gender'])\n",
    "            print('accent: ' + row['accent'])\n",
    "            print('locale: ' + row['locale'])\n",
    "            print('segment: ' + row['segment'])\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the original audio\n",
    "\n",
    "y, sr = librosa.load(selected_filename, sr=16000)\n",
    "\n",
    "# Perform time-stretching on the original audio\n",
    "\n",
    "y_time_stretched = y.copy()\n",
    "tmp_time_stretched = librosa.effects.time_stretch(y, np.random.uniform(0.9, 1.1))\n",
    "minlen = min(y.shape[0], tmp_time_stretched.shape[0])\n",
    "y_time_stretched *= 0\n",
    "y_time_stretched[0:minlen] = tmp_time_stretched[0:minlen]\n",
    "\n",
    "# Perform pitch-shifting on the original audio\n",
    "\n",
    "y_pitch_shifted = librosa.effects.pitch_shift(y, sr, n_steps=(4 * np.random.uniform()))\n",
    "\n",
    "y_louder = y * np.random.uniform(low=1.5, high=3.0)\n",
    "\n",
    "# Save these pre-processed files\n",
    "\n",
    "audio_time_stretched_filename = tempfile.NamedTemporaryFile(suffix='.wav').name\n",
    "audio_pitch_shifted_filename = tempfile.NamedTemporaryFile(suffix='.wav').name\n",
    "audio_louder_filename = tempfile.NamedTemporaryFile(suffix='.wav').name\n",
    "\n",
    "soundfile.write(audio_time_stretched_filename, y_time_stretched, sr, subtype='PCM_16')\n",
    "soundfile.write(audio_pitch_shifted_filename, y_pitch_shifted, sr, subtype='PCM_16')\n",
    "soundfile.write(audio_louder_filename, y_louder, sr, subtype='PCM_16')\n",
    "\n",
    "# Reload these pre-processed files in the format expected by\n",
    "# the pre-trained Mozilla DeepSpeech binary\n",
    "\n",
    "with wave.open(selected_filename, 'rb') as audio_input_file:\n",
    "    audio_input = np.frombuffer(\n",
    "        audio_input_file.readframes(\n",
    "            audio_input_file.getnframes()\n",
    "        ), np.int16\n",
    "    )\n",
    "\n",
    "with wave.open(audio_time_stretched_filename, 'rb') as audio_time_stretched_file:\n",
    "    audio_time_stretched = np.frombuffer(\n",
    "        audio_time_stretched_file.readframes(\n",
    "            audio_time_stretched_file.getnframes()\n",
    "        ), np.int16\n",
    "    )\n",
    "\n",
    "with wave.open(audio_pitch_shifted_filename, 'rb') as audio_pitch_shifted_file:\n",
    "    audio_pitch_shifted = np.frombuffer(\n",
    "        audio_pitch_shifted_file.readframes(\n",
    "            audio_pitch_shifted_file.getnframes()\n",
    "        ), np.int16\n",
    "    )\n",
    "\n",
    "with wave.open(audio_louder_filename, 'rb') as audio_louder_file:\n",
    "    audio_louder = np.frombuffer(\n",
    "        audio_louder_file.readframes(\n",
    "            audio_louder_file.getnframes()\n",
    "        ), np.int16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pre-build DeepSpeech model and scorer memory-mapped files\n",
    "\n",
    "ds = deepspeech.Model(os.path.join('./', 'deepspeech-0.9.3-models.pbmm'))\n",
    "ds.enableExternalScorer(os.path.join('./', 'deepspeech-0.9.3-models.scorer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Perform speech-to-text on the 4 audio clips\n",
    "\n",
    "audio_transcription_hypothesis = ds.stt(audio_input)\n",
    "audio_time_stretched_transcription_hypothesis = ds.stt(audio_time_stretched)\n",
    "audio_pitch_shifted_transcription_hypothesis = ds.stt(audio_pitch_shifted)\n",
    "audio_loudness_level_increased_transcription_hypothesis = ds.stt(audio_louder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output on the original audio\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of original audio: ' + audio_transcription_hypothesis)\n",
    "\n",
    "print('Original audio wave-plot:')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio_input.astype('float'), sr=sr)\n",
    "plt.show()\n",
    "\n",
    "Audio(audio_input, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output on time-stretched audio\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of time-stretched audio: ' + audio_time_stretched_transcription_hypothesis)\n",
    "\n",
    "print('Time-stretched audio wave-plot:')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio_time_stretched.astype('float'), sr=sr)\n",
    "plt.show()\n",
    "\n",
    "Audio(audio_time_stretched, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output on the pitch-shifted audio\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of pitch-shifted audio: ' + audio_pitch_shifted_transcription_hypothesis)\n",
    "\n",
    "print('Pitch-shifted audio wave-plot:')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio_pitch_shifted.astype('float'), sr=sr)\n",
    "plt.show()\n",
    "\n",
    "Audio(audio_pitch_shifted, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output on the audio with volume increased\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of louder audio: ' + audio_loudness_level_increased_transcription_hypothesis)\n",
    "\n",
    "print('Louder audio wave-plot:')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio_louder.astype('float'), sr=sr)\n",
    "plt.show()\n",
    "\n",
    "Audio(audio_louder, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline DeepSpeech model's output after passing through\n",
    "# LanguageTool\n",
    "\n",
    "language_tool = language_tool_python.LanguageTool('en-us')\n",
    "\n",
    "print('Ground truth sentence: ' + ground_truth_sentence)\n",
    "print('DeepSpeech transcription of after LanguageTool: ' + language_tool.correct(audio_transcription_hypothesis))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}